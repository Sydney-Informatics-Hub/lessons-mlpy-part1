{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA) for Regression\n",
    "\n",
    "> #### â˜‘ï¸Ž Objectives\n",
    "> - Introduce some of the key packages for EDA and ML\n",
    "> - Introduce and explore the Ames Housing dataset\n",
    "\n",
    "Here, we'll work through some exploration of the Ames housing data. Most of the code is provided for you,\n",
    "but you'll have to fill in some parts: these parts will be indicated with either `...` or `____`.\n",
    "\n",
    "First, letâ€™s load the required libraries. We will use the `sklearn` library for our ML tasks, and the `pandas`, `numpy`, `matplotlib`, `seaborn` and `upsetplot` libraries for general data processing and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries needed on colab\n",
    "!pip install upsetplot optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable some warnings produced by pandas etc.\n",
    "# (Don't do this in your actual analyses as you may miss important warnings)\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.simplefilter('ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import upsetplot\n",
    "%matplotlib inline\n",
    "sns.set(font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will use the Ames housing data to explore different ML approaches to regression. This dataset was â€œdesignedâ€ by Dean De Cock as an alternative to the â€œclassicâ€ Boston housing dataset, and has been extensively used in ML teaching. It is also available from kaggle as part of its [advanced regression practice competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\n",
    "\n",
    "The Ames Housing [Data Documentation file](https://github.sydney.edu.au/informatics/lessons-mlr/blob/gh-pages/_episodes_rmd/data/AmesHousing_DataDocumentation.txt) describes the independent variables presented in the data. This includes:\n",
    "\n",
    "- 20 continuous variables relate to various area dimensions for each observation\n",
    "- 14 discrete variables, which typically quantify the number of items occurring within the house\n",
    "- 23 ordinal, 23 nominal categorical variables, with 2 (STREET: gravel or paved) - 28 (NEIGHBORHOOD) classes\n",
    "\n",
    "We will explore both the â€œuncleanedâ€ data available from kaggle/UCI, and the processed data available in the AmesHousing package in R, for which documentation is available [here](https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf). It can be useful for understanding what each of the independent variables mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_clean = pd.read_csv('https://github.com/Sydney-Informatics-Hub/lessons-mlpy-flipped/raw/main/data/AmesHousingClean.csv')\n",
    "ames_raw = pd.read_csv('https://github.com/Sydney-Informatics-Hub/lessons-mlpy-flipped/raw/main/data/AmesHousingDirty.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Exploratory data analysis involves looking at:\n",
    "\n",
    "- the distribution of variables in your dataset\n",
    "- whether any data is missing\n",
    "- skewed\n",
    "- correlated variables\n",
    "\n",
    "\n",
    "> ### âš ï¸ Exercise\n",
    ">\n",
    "> Below, we'll explore the Ames housing dataset. As you go through the code,\n",
    "> think about:\n",
    ">\n",
    "> - What each variable represents and how it's been coded\n",
    "> - Which variables do you think will be important for predicting sale price?\n",
    "> - How do the \"clean\" and \"dirty\" datasets differ?\n",
    ">   - What was missing in the raw data?\n",
    ">   - What are some of the approaches that have been taken to deal with missingness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 5 rows of data with the `DataFrame.head()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks below:\n",
    "ames_clean.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.info()` to get information about the column types and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ames_clean.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the raw dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore how many null/missing values are in the clean dataset by using `isna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_clean.____().agg(\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about in the original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_raw.isna().agg(\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out which variables have the most missing data (we have to do a bit of extra work here to get this in a nice format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = (\n",
    "    ames_raw.isnull()\n",
    "    .agg('sum')\n",
    "    .pipe(lambda x: x[x > 0])  # Only show variables with > 0 missing values\n",
    "    .sort_values(ascending=False)\n",
    "    .reset_index()  # Convert to a dataframe\n",
    ")\n",
    "missing_counts.columns = [\"variable\", \"n_missing\"]\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the number of missing values with `seaborn`'s `barplot()` function. (Note: it's better to put `variable` on the y-axis so you can read the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (8, 10))\n",
    "# Fill in the columns to use as the x- and y-dimensions of the plot\n",
    "sns.barplot(missing_counts, y=..., x=..., ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"upset plot\" is a novel kind of plot for understanding what combinations occur in a dataset. Here, we'll use it for seeing which combinations of missing values are the most common. Why might these combinations occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use upsetplot to see where missing values occur\n",
    "# together\n",
    "# Only use the top 5 columns\n",
    "missing_cols = missing_counts[\"variable\"].iloc[:5].tolist()\n",
    "missing_combinations = (\n",
    "    ames_raw.loc[:, missing_cols]\n",
    "    .isnull()\n",
    "    .groupby(missing_cols)\n",
    "    .size()\n",
    ")\n",
    "upsetplot.plot(missing_combinations);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### âš ï¸ Challenge: Missingness\n",
    "> Compare the entries in the \"clean\" and \"dirty\" data frames (try `df['Column'].value_counts()`). How was the data cleaned up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try writing your own code here to compare them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Explore the data to see whether there are any unusual relationships between variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull out numeric and categoric variables:\n",
    "\n",
    "1. What data types do I have in my data? Can I infer that some of them are categorical, and others are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_clean.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pull out the categorical and numerical variables. Check the documentation for [select_dtypes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html) to see how you can select both `float` and `int` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ames_clean.select_dtypes(include = ['object']).columns.tolist()\n",
    "numeric_vars = ames_clean.select_dtypes(include = [...]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the numeric variables with `sns.pairplot`, and their relationship with sale price\n",
    "\n",
    "> ## âš ï¸ Challenge\n",
    "> Look at the documentation for [pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) and try out some of the different visualisation options.\n",
    "> Do you prefer any of them for understanding this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot ~10 variables at a time - otherwise too hard to\n",
    "#   see what's happening\n",
    "sns.pairplot(\n",
    "    ames_clean,\n",
    "    vars = numeric_vars[:10] + [\"Sale_Price\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot remaining variables\n",
    "sns.pairplot(\n",
    "    ames_clean,\n",
    "    diag_kind=\"kde\",\n",
    "    vars = numeric_vars[10:20] + [\"Sale_Price\"],\n",
    "    diag_kws={'bw': 0.01}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "\n",
    "`pandas` has a builtin method for calculating the [correlations](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) between all variables in a DataFrame.\n",
    "\n",
    "Of the numeric variables, which ones are the most correlated:\n",
    "* With sale price?\n",
    "* With each other? (This is an important thing to check for some models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = ames_clean[numeric_vars].____()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(correlations,\n",
    "            cmap=plt.cm.BrBG, \n",
    "            vmin=-0.5, vmax=0.5, \n",
    "            square=True,\n",
    "            xticklabels=True, yticklabels=True,\n",
    "            ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use clustering to group highly correlated variables together.\n",
    "\n",
    "* What groups of variables emerge?\n",
    "* Does it make sense that these variables would be correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(\n",
    "    correlations, \n",
    "    method=\"ward\", \n",
    "    cmap=plt.cm.BrBG, \n",
    "    vmin=-0.5, vmax=0.5,\n",
    "    xticklabels=1, yticklabels=1,\n",
    "    figsize=(12, 12)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to examine the relationship between a variable and sale price in more detail, you can use plots like `sns.lmplot`.\n",
    "\n",
    "Let's look at `Gr_Liv_Area`, since it had an odd distribution in the scatterplots above:\n",
    "\n",
    "* What do you notice about the distribution?\n",
    "* Is there a group of values that stand out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    data = ames_clean,\n",
    "    x = 'Gr_Liv_Area', y = 'Sale_Price'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see any outliers in the data? Arguably, the five points on the right, wtih Gr_Liv_Area > 4000, are outliers, that can significantly affect the fit of our model. We will filter these out prior to modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_clean = ames_clean.loc[ames_clean['Gr_Liv_Area'] <= 4000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use violin plots (a variation on box plots that also shows the distribution of the values) to explore the relationship between categorical variables and the outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a categorical variable to plot against sale price\n",
    "f, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "sns.violinplot(\n",
    "    x = ...,\n",
    "    y = 'Sale_Price',\n",
    "    data = ames_clean,\n",
    "    ax = ax\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore distribution of Outcome variable\n",
    "\n",
    "You also need to do EDA on the outcome variable to:\n",
    "\n",
    "- identify outliers\n",
    "- explore whether there is any skew in its distribution, and possibly identify a transformation to use when modelling the data\n",
    "\n",
    "This is because many models, including ordinary linear regression, assume that prediction errors (and hence the response) are normally distributed.\n",
    "\n",
    "Plot the distribution of the sale price variable using `sns.distplot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (10, 6))\n",
    "sns.distplot(\n",
    "    # Note: this function doesn't take a data argument,\n",
    "    #   you have to provide the variable as df['column']\n",
    "    ...,\n",
    "    kde = False,\n",
    "    ax = ax\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore different transformations of the sale price to see which transformation makes the outcome look more normally distributed. The [qqplot](https://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot.html) function from `statsmodels` plots values against a line showing where they would be expected to fall if the data was normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "sm.____(\n",
    "    ames_clean['Sale_Price'],\n",
    "    line = 's'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqrt transformation\n",
    "sm.qqplot(\n",
    "    np.sqrt(ames_clean['Sale_Price']),\n",
    "    line = 's'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform\n",
    "sm.qqplot(\n",
    "    np.log(ames_clean['Sale_Price']),\n",
    "    line = 's'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log10 transform\n",
    "sm.qqplot(\n",
    "    np.log10(ames_clean['Sale_Price']),\n",
    "    line = 's'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### âš ï¸ Challenge: Outcome transformations\n",
    "> If you were working with this dataset, which of the above would you prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ðŸ“ Key points: Exploratory Data Analysis\n",
    "> - EDA is the first step of any analysis, and often very time consuming.\n",
    "> - Skipping EDA can result in substantial issues with subsequent analysis.\n",
    "> \n",
    "> #### Questions:\n",
    "> - How do we predict one continuous variable based on others?\n",
    "> - What is the first step of any ML project (and often the most time consuming)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models: Linear regression\n",
    "\n",
    "> ## â“ Questions\n",
    "> - How do we preprocess our data for modelling?\n",
    "> - How do we fit a basic linear model using scikit-learn?\n",
    "    \n",
    "> ## â˜‘ï¸Ž Objectives\n",
    "> - To use sklearn.preprocessing or pandas to preprocess our data\n",
    "> - To fit and compare some basic linear models using one, two, \n",
    "    or all of the variables in the dataset\n",
    "\n",
    "With the initial data exploration out of the way, we're going to start fitting predictive models to predict the sale price.\n",
    "\n",
    "First we'll import some new libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import exp10\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import pickle \n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output = \"pandas\")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, PolynomialFeatures, FunctionTransformer, OneHotEncoder\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to pick up from this point of the workshop, we'll reload the data (and drop the rows with outlying values of `Gr_Liv_Area`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_clean = pd.read_csv('https://github.com/Sydney-Informatics-Hub/lessons-mlpy-flipped/raw/main/data/AmesHousingClean.csv')\n",
    "ames_raw = pd.read_csv('https://github.com/Sydney-Informatics-Hub/lessons-mlpy-flipped/raw/main/data/AmesHousingDirty.csv')\n",
    "ames_clean = ames_clean.loc[ames_clean['Gr_Liv_Area'] <= 4000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the models we will fit, we need to do some preprocessing of our data to code the variables in the ways the models expect.\n",
    "Standard preprocessing steps in machine learning include:\n",
    "\n",
    "* Encoding categorical variables as numeric values - for nominal variables, we often use **one hot** or **dummy** coding.\n",
    "* Scaling numeric variables to have the same mean and standard deviation - some models need this more than others.\n",
    "\n",
    "We don't just want to apply a standard recipe, however, we also do some preprocessing based on our understanding of the data. In this dataset,\n",
    "some important features that aren't currently encoded include:\n",
    "\n",
    "* Some houses may not have basements or second floors - they will have values of 0 for variables like basement area, but we want to explicitly encode whether they have a basement or not\n",
    "* Some categorical variables may have rare values that will not be useful for pediction\n",
    "* There are features for the year a property was sold or remodelled, but not \n",
    "\n",
    "To start encoding these variables, let's first separate the categorical variables from the numeric ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ames_clean.select_dtypes(include = ['object']).columns.tolist()\n",
    "numeric_vars = ames_clean.select_dtypes(include = ['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we do some preprocessing:\n",
    "\n",
    "- **dummy code** the categorical variables using `sklearn`'s `OneHotEncoder` - it also has the option to **group uncommon categories** into a single category.\n",
    "- For features like `Second_Flr_SF`, create separate **binary** variables that explicitly encode whether the property has a second floor or not\n",
    "- For features like `Year_Built`, recode them into the age/number of years since they were built\n",
    "\n",
    "`sklearn` has a few preprocessing tools, and in theory we could do all of our preprocessing using them, but in practice it's easier to use a mix of `sklearn`'s tools and standard `pandas` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_feature(df, column: str):\n",
    "    return np.where(df[column] == 0, 0, 1)\n",
    "\n",
    "# Age variables\n",
    "ames_clean['Age'] = ames_clean['Year_Sold'].max() - ames_clean['Year_Built']\n",
    "ames_clean['Remodel_Age'] = ames_clean['Year_Sold'].max() - ames_clean['Year_Remod_Add']\n",
    "\n",
    "# Any misc features vs. none\n",
    "ames_clean['Misc_Feature_Present'] =  np.where(ames_clean['Misc_Feature'] == \"None\", 0, 1)\n",
    "\n",
    "# Dummy code categorical variables, and group infrequent categories together\n",
    "onehot_encoder = OneHotEncoder(min_frequency=20, sparse_output=False, drop=\"first\")\n",
    "dummy_vars = onehot_encoder.fit_transform(ames_clean[cat_vars])\n",
    "# Drop original categorical vars\n",
    "ames_clean = ames_clean.drop(columns=cat_vars)\n",
    "# Add dummy_vars\n",
    "ames_clean = pd.concat([ames_clean, dummy_vars], axis=1)\n",
    "\n",
    "\n",
    "# Create binary features for second floor etc.\n",
    "binary_features = [\n",
    "    'Second_Flr_SF','Three_season_porch','BsmtFin_SF_2','Bsmt_Unf_SF',\n",
    "    'Enclosed_Porch','Low_Qual_Fin_SF', 'Mas_Vnr_Area','Lot_Frontage',\n",
    "    'Open_Porch_SF','Screen_Porch','Pool_Area','Wood_Deck_SF', \n",
    "]\n",
    "for column in binary_features:\n",
    "    ames_clean[column + \"_binary\"] = create_binary_feature(ames_clean, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing above does create some duplicate variables, e.g. there are multiple columns indicating `No_Basement` (because there were multiple categorical variables with this value).\n",
    "\n",
    "We also want to drop the `Year_Built` and `Year_Remod_Add` variables, since we've replaced them with ages.\n",
    "\n",
    "We'll drop these variables (except for the first occurrence of each duplicate) now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basement_vars = list(ames_clean.filter(regex=\"No_Basement\").columns[1:])\n",
    "garage_vars = list(ames_clean.filter(regex=\"No_Garage\").columns[1:])\n",
    "vars_to_drop = [*basement_vars, *garage_vars, \"Year_Built\", \"Year_Remod_Add\"]\n",
    "print(\"Dropping:\", vars_to_drop)\n",
    "ames_clean = ames_clean.drop(columns=vars_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting: creating training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn`'s `train_test_split` to create training and testing datasets. We want similar distributions of the outcome in the training and test data, so we can use the `stratify` option. However, this requires a categorical variable, so we'll first bin the `Sale_Price` outcome to create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut into 10 bins\n",
    "ames_clean['Sale_Price_quantile'] =  pd.qcut(\n",
    "    ames_clean['Sale_Price'],\n",
    "    q=...,\n",
    "    labels=range(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data, allocating 70% of the data to the training set (and 30% to the test set). See [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for how to specify the size of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_test  = train_test_split(\n",
    "    ames_clean.index.values, \n",
    "    train_size=...,\n",
    "    stratify = ames_clean['Sale_Price_quantile'].values,\n",
    "    # We use random_state to get consistent results each time we run this\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# get rid of the quartile column so we're not using it to predict sale price\n",
    "ames_clean = ames_clean.drop('Sale_Price_quantile', axis = 1)\n",
    "\n",
    "# Create variables for the training and test sets \n",
    "ames_train = ames_clean.loc[index_train,:].copy()\n",
    "ames_test = ames_clean.loc[index_test,:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows have been allocated to the training and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows + columns in each dataset\n",
    "print(ames_train.____)\n",
    "print(ames_test.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log-transform the outcome to a more normal distribution, we can use `sklearn`'s `FunctionTransformer`: while we could just transform the outcome ourself, `FunctionTransformer` allows us to transform the response back to the original scale easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want numpy's log10 function for the transformation\n",
    "log_transformer = FunctionTransformer(\n",
    "    func=...,\n",
    "    inverse_func=exp10,\n",
    "    validate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to separate the predictors ($X$) from the outcome ($y$), for both the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ames_clean.columns.values.tolist()\n",
    "predictors.remove('Sale_Price')\n",
    "# Create training and test response vectors\n",
    "ames_train_y = log_transformer.fit_transform(ames_train[['Sale_Price']])\n",
    "ames_test_y = log_transformer.transform(....[['Sale_Price']])\n",
    "\n",
    "# Write training and test design matrices\n",
    "ames_train_x = ames_train[predictors].copy()\n",
    "ames_test_x = ames_test[predictors].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the types of the $X$ and $y$ variables: a `DataFrame` works for the predictors, but `sklearn` generally expects a single array for the outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ames_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting linear regression models\n",
    "\n",
    "Now that we've created the training and test datasets, we're ready to fit our first models.\n",
    "We'll start with a simple one: a linear regression model, using all our predictors. We will fit this to the **training** data to fit the coefficients, and later we'll **predict** on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an Ordinary Least Squares Regression using all variables\n",
    "# fit_intercept = True is the default option,\n",
    "# we don't need to specify it in other regressions \n",
    "ames_ols_all = LinearRegression(fit_intercept=True)\n",
    "ames_ols_all.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an explanatory context, we might have predicted sale price using a smaller set of variables that we expected/theorized to be important, e.g. the age of the house. We can fit a model using only a few variables to see how this strategy might work in a prediction context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vars = ['Age', 'First_Flr_SF', 'Second_Flr_SF']\n",
    "ames_ols_reduced = LinearRegression()\n",
    "ames_ols_reduced.fit(ames_train_x[...], ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of linear regression is that the coefficients are interpretable (although here the represent changes on the log-scale). So we can easily see whether each predictor has a positive or negative relationship with the outcome.\n",
    "\n",
    "We can inspect the intercept and coefficients from the models:\n",
    "\n",
    "> ### âš ï¸ Challenge\n",
    "> In the reduced model, do age, first floor area and second floor area have positive or negative relationships with the outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_ols_reduced.intercept_, ames_ols_reduced.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing model fit:\n",
    "\n",
    "`sklearn` provides a number of useful tools for predicting from our models, and then applying different accuracy metrics to those predictions.\n",
    "\n",
    "The core method that we will use here is `model.predict()`, which allows us to apply the fitted model to a dataset to get the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values on the training dataset\n",
    "ols_preds_log = ames_ols_all.____(ames_train_x)\n",
    "# Remember these will be on the log scale: we can transform them back to the original scale\n",
    "ols_preds = log_transformer.inverse_transform(ols_preds_log)\n",
    "ols_preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, generating a full table of results for multiple metrics requires some tedious manual work. To save time, we've created a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_fit_vars(models, variables, datasetX, datasetY):\n",
    "    columns = ['RMSE', 'R2', 'MAE']\n",
    "    results = pd.DataFrame(0.0, columns=columns, index=variables)\n",
    "    # compute the actual Y\n",
    "    y_actual = log_transformer.inverse_transform(datasetY)\n",
    "    for i, method in enumerate(models):\n",
    "        if variables[i] != \"All\":\n",
    "            tmp_dataset_X = datasetX[variables[i]]\n",
    "            if type(variables[i]) == str: #only one column - so need to reshape\n",
    "                tmp_dataset_X = datasetX[variables[i]].values.reshape(-1, 1)\n",
    "        else:\n",
    "            tmp_dataset_X = datasetX\n",
    "        # while we build the model and predict on the log10Transformed sale price,\n",
    "        # we display the error in dollars as that makes more sense\n",
    "        y_pred = log_transformer.inverse_transform(method.predict(tmp_dataset_X))\n",
    "        results.iloc[i,0] = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "        results.iloc[i,1] = r2_score(y_actual, y_pred)\n",
    "        results.iloc[i,2] = mean_absolute_error(y_actual, y_pred)\n",
    "    return results.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the accuracy measures for the two fitted models **on the training data**:\n",
    "\n",
    "> ### âš ï¸ Challenge\n",
    "> We have compared the models using root mean-squared error, R^2 and mean absolute error.\n",
    "> For each of these measures, are higher or lower values better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ames_ols_all, ames_ols_reduced]\n",
    "variables = [\"All\", reduced_vars]\n",
    "\n",
    "compare_train = assess_fit_vars(\n",
    "    models=models,\n",
    "    variables=variables,\n",
    "    # We want to predict from the training data here\n",
    "    datasetX=..., \n",
    "    datasetY=ames_train_y\n",
    ")\n",
    "compare_train.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot these measures. Rearranging the data for plotting also requires a bit of work, so we've written a function in advance to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_df(df):\n",
    "    out_df = (\n",
    "        df.copy()\n",
    "        .reset_index()\n",
    "        .melt(\n",
    "            id_vars='index',\n",
    "            value_vars=df.columns.values.tolist(),\n",
    "            var_name='metric',\n",
    "            value_name='number'\n",
    "        )\n",
    "        .sort_values('number')\n",
    "    )\n",
    "    out_df['index'] = out_df['index'].astype(str)\n",
    "    out_df= out_df.rename(columns={'index':'model_features'})\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    x='model_features',\n",
    "    y='number',\n",
    "    col='metric',\n",
    "    # Rearrange the results to plot them\n",
    "    data=...,\n",
    "    kind='bar',\n",
    "    sharey=False,\n",
    ")\n",
    "chart.set_xticklabels(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on the test set\n",
    "\n",
    "Remember that the most important goal for our model is predicting on **new data** that wasn't used to train the model: here, the test set.\n",
    "\n",
    "We'll predict on the test set - how does the accuracy compare to the training set? Why might there be differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_test = assess_fit_vars(\n",
    "    models=models,\n",
    "    variables=variables,\n",
    "    # Provide the test data here\n",
    "    datasetX=...,\n",
    "    datasetY=ames_test_y\n",
    ")\n",
    "compare_test.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine the training and test scores and plot them to compare them easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(train_scores, test_scores):\n",
    "    df = rearrange_df(train_scores)\n",
    "    df['dataset'] = 'train'\n",
    "    df1 = rearrange_df(test_scores)\n",
    "    df1['dataset'] = 'test'\n",
    "    return pd.concat([df, df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    x='model_features',\n",
    "    y='number',\n",
    "    col='metric',\n",
    "    data=combine_results(compare_train, compare_test),\n",
    "    kind='bar',\n",
    "    sharey=False,\n",
    "    # Colour by dataset to compare test/training\n",
    "    hue=...\n",
    ")\n",
    "chart.set_xticklabels(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## ðŸ“ Key points: Linear regression\n",
    "> - Regression is the prediction of the value of a continuous \n",
    "> variable based on one or more other continuous or categorical variables.\n",
    "> - Multiple types of regression can be implemented to fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional extra: interactions\n",
    "\n",
    "One of the drawbacks of linear regression models is that, without transforming the original predictors, they can only model linear relationships between variables. However, we can create **interaction terms** that combine multiple variables, and potentially capture more complex relationships between those variables.\n",
    "\n",
    "We'll use `sklearn` to create an interaction between the ground floor living area and second floor square footage. This is a little bit harder to integrate with \n",
    "our previous models (as we have to create a dataset with a new interaction term in it),\n",
    "so it has been kept separate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction term (not polynomial features) \n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True) \n",
    "X_inter = interaction.fit_transform(ames_train_x[['Gr_Liv_Area','Second_Flr_SF']])\n",
    "X_inter = pd.DataFrame(\n",
    "    X_inter, \n",
    "    columns = interaction.get_feature_names_out(['Gr_Liv_Area','Second_Flr_SF'])\n",
    ")\n",
    "ames_ols_GrLivArea_Second_Flr_SF_interaction = LinearRegression() \n",
    "ames_ols_GrLivArea_Second_Flr_SF_interaction.fit(X_inter, ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example of how to create interactions, so we don't necessarily expect great accuracy, but we could potentially create interactions between all pairs of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training performance\n",
    "assess_fit_vars(\n",
    "    models=[ames_ols_GrLivArea_Second_Flr_SF_interaction], \n",
    "    variables=[\"All\"], \n",
    "    datasetX=X_inter, datasetY=ames_train_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized regression: Lasso, Ridge, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries we need for this section\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Disable some distratcting warnings: don't do this in an\n",
    "#   actual analysis unless you're sure you understand\n",
    "#   the warnings you see!\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning: selecting the optimal value of lambda\n",
    "\n",
    "Recall that both ridge and lasso regression have an additional parameter, lambda, which captures the penalty for including more (non-zero) features in the model. \n",
    "\n",
    "\n",
    "To fit these models, we need to first find the optimal value of lambda (using cross-validation), and then fit the model, and assess its fit.\n",
    "\n",
    "Also, for both ridge and lasso regression, the **scale** of the variables matters (because the penalty term in the objective function treats all coefficients as comparable!). So we have to use the `StandardScaler()` function to standardize all numeric variables.\n",
    "\n",
    "We will do all of this in a scikit-learn **pipeline**. Pipelines can include multiple steps, potentially including all your preprocessing and feature engineering. However, as discussed above, for this dataset we found that it was easiest to do *some* of the preprocessing \"manually\" with custom code. \n",
    "\n",
    "Some advantages of pipelines are:\n",
    "\n",
    "* They make it easy to ensure you're not accidentally **leaking information** from the test dataset when training your model. In short:\n",
    "  * Use `pipeline.fit_transform(training_data)` on your **training data**: the **transform** part means that the parameters used by the pipeline (e.g. the means and standard deviations of variables, for `StandardScaler`), will be updated from the data.\n",
    "  * Use `pipeline.fit(test_data)` on you **test data**: the steps of the pipeline are only fitted to your data, using the existing parameter values\n",
    "* You can keep fewer intermediate datasets around, e.g. you don't have to create separate, scaled copies of the data if you do the scaling within the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression (L2 regularisation)\n",
    "\n",
    "The main hyperparameter that needs to be tuned in ridge regression is the size of the penalty applied to the coefficients, which is refeerred to as `alpha` in `sklearn`.\n",
    "\n",
    "We will generate a range of possible `alpha` values and find the best one using cross-validation.\n",
    "\n",
    "We'll use `GridSearchCV` here, which is generic and works for all models. However,\n",
    "for ridge regression there is a specialized algorithm that can be used to do cross-validation\n",
    "faster (`RidgeCV`), which you should use in actual workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a wide range of alpha values - best to search in log-space\n",
    "#   since we want to test across a wide range\n",
    "alphas = list(np.logspace(start=-12, stop=10, num=20, base=2))\n",
    "params = {\n",
    "    \"alpha\": alphas,\n",
    "}\n",
    "\n",
    "# Set up our training pipeline\n",
    "ames_ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    # Provide the parameters to the cross-validation step\n",
    "    ('estimator', GridSearchCV(\n",
    "        ElasticNet(l1_ratio=0), \n",
    "        param_grid=params, \n",
    "        # Set the number of cross-validation folds (we'll use 5 for this workshop)\n",
    "        cv=...,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "ames_ridge.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running cross-validation, we can access the final selected parameter value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to select the 'estimator' step to access the cross-validation outputs:\n",
    "print(\"Final alpha value:\", ames_ridge[...].best_params_[\"alpha\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing cross-validation performance\n",
    "\n",
    "In addition to the final selected model and parameters, we can also access the performance metrics for each value of the parameter that was tried, and visualize how performance changed across the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = pd.DataFrame(ames_ridge[\"estimator\"].cv_results_)\n",
    "ridge_cv[\"mean_squared_error\"] = ridge_cv[\"mean_test_score\"] * -1\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    data=ridge_cv,\n",
    "    # We want to plot across the values of alpha (\"param_alpha\" in the cv_results)\n",
    "    x=...,\n",
    "    y=\"mean_squared_error\",\n",
    ")\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso (L1 regularisation)\n",
    "\n",
    "The lasso model accepts the same parameters and is set up in the same way as the ridge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"alpha\": alphas,\n",
    "}\n",
    "\n",
    "ames_lasso = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    # Provide the parameters to the cross-validation step\n",
    "    ('estimator', GridSearchCV(\n",
    "        # We want to do 100% L1 regularisation here, so set l1_ratio to:\n",
    "        ElasticNet(l1_ratio=...), \n",
    "        param_grid=params, \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "\n",
    "ames_lasso.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the best value of alpha (the penalty parameter for Lasso regression?)\n",
    "best_alpha_lasso = ames_lasso[\"estimator\"].best_params_[\"alpha\"]\n",
    "print(best_alpha_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net (combining L1 and L2 regularisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": np.linspace(0.0, 1.0, num=10),\n",
    "}\n",
    "\n",
    "ames_enet = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    # Provide the parameters to the cross-validation step\n",
    "    ('estimator', GridSearchCV(\n",
    "        ElasticNet(), \n",
    "        param_grid=..., \n",
    "        cv=5,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "ames_enet.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameter values \n",
    "best_params_enet = ames_enet[\"estimator\"].best_estimator_\n",
    "print(best_params_enet.alpha, best_params_enet.l1_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to visualise performance across the different parameters again - this time we need to visualise two parameters simultaneously, so we need to use another dimension like colour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the tuning performance\n",
    "enet_results = pd.DataFrame.from_dict(ames_enet[\"estimator\"].cv_results_)\n",
    "enet_results[\"mean_squared_error\"] = enet_results[\"mean_test_score\"] * -1\n",
    "from matplotlib.colors import LogNorm\n",
    "ax = sns.lineplot(data=enet_results, \n",
    "                  x='param_l1_ratio', \n",
    "                  y='mean_squared_error',\n",
    "                  hue='param_alpha',\n",
    "                  palette=plt.cm.viridis,\n",
    "                  hue_norm=LogNorm())\n",
    "ax.set_xlabel(\"Mixing proportion\")\n",
    "ax.set_ylabel('Mean Squared Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## âš ï¸ Challenge: Elastic Net\n",
    ">\n",
    "> 1. Look at the coefficients for the model above. What was the balance between L1 (Lasso) and L2 (Ridge) regression?\n",
    "> 2. What value of alpha was found to be optimal? Was this value expected based on the results we got when we ran Lasso and Ridge independently?\n",
    "\n",
    "See [this post on Stack Overflow](https://stackoverflow.com/questions/47365978/scikit-learn-elastic-net-approaching-ridge) for more context on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A similar function has already been defined, but this\n",
    "## is generic to all models\n",
    "def assess_model_fit(models,\n",
    "                     model_labels, \n",
    "                     datasetX, \n",
    "                     datasetY):\n",
    "    columns = ['RMSE', 'R2', 'MAE']\n",
    "    rows = model_labels\n",
    "    results = pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "    for i, method in enumerate(models):\n",
    "        tmp_dataset_X = datasetX\n",
    "        # while we build the model and predict on the log10Transformed \n",
    "        # sale price, we display the error in dollars as that makes more sense\n",
    "        y_pred = log_transformer.inverse_transform(method.predict(tmp_dataset_X).reshape(-1, 1))\n",
    "        y_actual = log_transformer.inverse_transform(datasetY)\n",
    "        results.iloc[i,0] = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "        results.iloc[i,1] = r2_score(y_actual, y_pred)\n",
    "        results.iloc[i,2] = mean_absolute_error(y_actual, y_pred)\n",
    "    return(results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the RMSE on the training data?\n",
    "assess_model_fit(models = [ames_ols_all, ames_ridge, ames_lasso, ames_enet],\n",
    "                 model_labels =['OLS','Ridge', 'Lasso', \"ENet\"], \n",
    "                 datasetX=ames_train_x,\n",
    "                 datasetY=ames_train_y).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the coefficients of the models to see which variables are important to prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(model, labels):\n",
    "    # Support both regression/regularised regression and \n",
    "    #   random forest models\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coefs = model.coef_.ravel()\n",
    "        title = 'Estimated coefficients'\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        coefs = model.feature_importances_\n",
    "        title = 'Feature importances'\n",
    "    table = pd.Series(coefs, index = labels)\n",
    "    # Get the largest 20 values (by absolute value)\n",
    "    table = table[table.abs().nlargest(20).index]\n",
    "\n",
    "    fig, ax = fig, ax = plt.subplots()\n",
    "    table.T.plot(kind='barh', edgecolor='black', width=0.7, linewidth=.8, alpha=0.9, ax=ax)\n",
    "    ax.tick_params(axis=u'y', length=0) \n",
    "    ax.set_title(f'{title} (twenty largest in absolute value)', fontsize=14)\n",
    "    sns.despine()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefficients(ames_ols_all, predictors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefficients(ames_enet[\"estimator\"].best_estimator_, predictors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## âš ï¸ Challenge: Coefficients\n",
    "> Compare the top coefficients for the models above. Why do you \n",
    "> think the top/bottom predictors are different for each one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ðŸ“ Key points: Regularized regression and dimension reduction\n",
    "> - There are many extensions to the basic regression approach \n",
    "    which can enable a better fit on the data.\n",
    "> - Regularisation helps us improve the performance of regression by reducing the impact of less-useful variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regression. K nearest neighbor regression\n",
    "\n",
    "> ## â“Questions\n",
    "> - How do we implement tree-based and distance-based methods in Python?\n",
    ">\n",
    "> ## â˜‘ï¸Ž Objectives\n",
    "> - Fit a RF and KNN model to our data\n",
    "> - Explore the effect of hyperparameters on model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forest, each tree in the ensemble is built from a bootstrap sample from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is the best split among a random subset of the features. The hyperparameters\n",
    "control how this is done.\n",
    "\n",
    "See the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for more details on the parameters.\n",
    "\n",
    "An example parameter grid for a random forest model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning grid was defined to optimise the following RF parameters:\n",
    "param_grid = {\"n_estimators\": list(np.arange(10, 160, 10)),\n",
    "            'max_depth': list(np.arange(3, 11, 1)),\n",
    "            'min_samples_split': [0.005, 0.01, 0.02],\n",
    "             'max_features': ['sqrt', 1.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was optimised in advance, and the best outcome of this ended up being:\n",
    "\n",
    "```\n",
    "{'max_depth': 9, 'min_samples_split': 0.005, \n",
    "'max_features': 1.0, 'n_estimators': 150}\n",
    "\n",
    "# best score\n",
    "0.8735794018428228\n",
    "```\n",
    "\n",
    "Tuning a grid of multiple parameters (and multiple values for each parameter) can take a long time:\n",
    "each new parameter value **multiplies** the number of models that have to\n",
    "be fitted.\n",
    "\n",
    "For the sake of time, we won't actually tune the model here, but see below for some tips\n",
    "on tuning large parameter grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_rf = Pipeline([\n",
    "    # Note: random forest models don't require variables to be\n",
    "    #   scaled, so we can skip that step\n",
    "    ('estimator', RandomForestRegressor(n_estimators=150, \n",
    "                                       max_depth = 9,\n",
    "                                       min_samples_split = 0.005,\n",
    "                                       max_features = 1.0))\n",
    "    # If we want to actually tune these parameters\n",
    "    # ('estimator', GridSearchCV(RandomForestRegressor(), param_grid, scoring='r2', cv=5, n_jobs=-1))\n",
    "])\n",
    "\n",
    "ames_rf.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## â†”ï¸ Extension: Tuning large parameter grids\n",
    ">\n",
    "> If you do want to do a grid search over multiple parameters, here are some tips/strategies:\n",
    ">\n",
    "> - Use **parallel processing**: `GridSearchCV` has a `n_jobs` argument that allows you to fit\n",
    ">   multiple models in parallel: `n_jobs=-1` uses all your computer's cores. If you have a\n",
    ">   modern computer with multiple CPU cores and plenty of RAM (memory), you may be able\n",
    ">   to tune fairly large grids this way\n",
    "> - Use the university's [High Performance Computing](https://www.sydney.edu.au/research/facilities/sydney-informatics-hub/digital-research-infrastructure.html)\n",
    ">   facilities, e.g. Artemis, for jobs that you can't tackle with your local computer.\n",
    "> - Some models (e.g. `xgboost`) may be able to make use of GPUs for much faster parallel\n",
    ">   processing - but this can be hard to set up. Aim to train on a GPU only if you're\n",
    ">   sure you can't use a CPU for your model.\n",
    ">\n",
    "> We'll try to show an example of tuning a large parameter grid below in the **xgboost** section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest: coefficients/feature importances\n",
    "\n",
    "Random forest models have a specialized method for calculating the importance of each feature for prediction, based on \"impurity\" - roughly how well the feature separates the data into distinct groups when it is used in a split. We can show that below using our existing\n",
    "`plot_coefficients` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coefficients(ames_rf['estimator'], predictors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> â„¹ï¸ The plot above shows the impurity-based feature importances, which are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree.\n",
    "\n",
    "One disadvantage of impurity-based feature importances is that this can be biased towards features with many unique values (high cardinality). To overcome this bias one can perform a permutation feature importance as shown below (this might take a bit longer to run)\n",
    "\n",
    "> ðŸ—’ï¸  Note that feature importance based on permutations are model-agnostic and can be applied for other models as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_res = permutation_importance(ames_rf, ames_train_x, ames_train_y, n_repeats=5, random_state=0, n_jobs=2)\n",
    "corrcoef_rf = np.asarray(perm_res.importances_mean)\n",
    "corrcoef_rf_std = np.asarray(perm_res.importances_std)\n",
    "#Select 20 largest only\n",
    "sorted_idx = corrcoef_rf.argsort()[-20:]\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "bar = ax.barh(np.arange(20), corrcoef_rf[sorted_idx], xerr = corrcoef_rf_std[sorted_idx],\n",
    " tick_label = np.asarray(predictors)[sorted_idx], align='center')\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours Regression\n",
    "\n",
    "Again, for the sake of time, we will not tune the k-nearest neighbours model over a large grid. The parameters below show an example of how you might tune the parameters over a larger grid.\n",
    "\n",
    "In actual analyses, you want to search over a large grid to ensure you've found the optimal\n",
    "parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid that searches over a range of n_neighbors\n",
    "param_grid = {\n",
    "    \"n_neighbors\": list(np.arange(3,21,2)),\n",
    "    \"weights\": ['uniform','distance'],\n",
    "}\n",
    "\n",
    "# Results from tuning:\n",
    "# print(ames_kNN.named_steps['estimator'].best_score_)\n",
    "# 0.7842456772785913\n",
    "# KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',metric_params=None, n_jobs=1, n_neighbors=7, p=2, weights='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll just tune over a smaller grid (centred around the optimal value for `n_neighbors` we found in advance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's try to tune locally, trying 6, 7 and 8 neighbors:\n",
    "param_grid = {\"n_neighbors\": [6,7,8],\n",
    "              \"weights\": ['uniform']}\n",
    "\n",
    "\n",
    "ames_knn = Pipeline([\n",
    "    # KNN models are strongly affected by scaling, so add the scaler step\n",
    "    #  again\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', GridSearchCV(KNeighborsRegressor(), \n",
    "                               param_grid, scoring='r2', cv=10))\n",
    "])\n",
    "\n",
    "ames_knn.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ames_knn['estimator'].best_estimator_)\n",
    "print(ames_knn['estimator'].best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_train = assess_model_fit(\n",
    "    models=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, \n",
    "            ames_rf, ames_knn],\n",
    "    model_labels=['OLS','Ridge', 'Lasso', 'ENet', 'RF', 'kNN'],\n",
    "    datasetX=ames_train_x,\n",
    "    datasetY=ames_train_y)\n",
    "compare_train.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    x='model_features',\n",
    "    y='number',\n",
    "    col='metric',\n",
    "    data=rearrange_df(compare_train),\n",
    "    kind='bar',\n",
    "    sharey=False,\n",
    ")\n",
    "chart.fig.set_size_inches((10, 6))\n",
    "chart.set_xticklabels(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_test = assess_model_fit(\n",
    "    models=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, \n",
    "            ames_rf, ames_knn],\n",
    "    model_labels=['OLS','Ridge', 'Lasso', 'ENet', 'RF', 'kNN'],\n",
    "    datasetX=ames_test_x,\n",
    "    datasetY=ames_test_y)\n",
    "compare_test.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    x='model_features',\n",
    "    y='number',\n",
    "    col='metric',\n",
    "    data=rearrange_df(compare_test),\n",
    "    kind='bar',\n",
    "    sharey=False,\n",
    ")\n",
    "chart.fig.set_size_inches((10, 6))\n",
    "chart.set_xticklabels(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.catplot(\n",
    "    x='model_features',\n",
    "    y='number',\n",
    "    col='metric',\n",
    "    data=combine_results(compare_train, compare_test),\n",
    "    kind='bar',\n",
    "    sharey=False,\n",
    "    hue='dataset'\n",
    ")\n",
    "chart.fig.set_size_inches((12, 6))\n",
    "chart.set_xticklabels(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## ðŸ“ Key points: Random forests and distance-based methods\n",
    "> - Random forests can be combined to solve regression tasks\n",
    "> - kNN is a method that can also be used for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting methods: XGBoost\n",
    "\n",
    "> ## â“Questions\n",
    "> - What are some newer approaches to ML?\n",
    "> - What are their pros and cons?\n",
    ">\n",
    "> ## â˜‘ï¸Ž Objectives\n",
    "> - Explore how to optimise ML models with\n",
    ">   a much larger number of parameters\n",
    "> - Learn strategies for dealing with large models and\n",
    ">   large parameter spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting and XGBoost\n",
    "\n",
    "Models like `XGBoost` can achieve high accuracy\n",
    "on a wide range of datasets, but they are complex\n",
    "models with a large number of hyperparameters\n",
    "governing their behaviour.\n",
    "\n",
    "With a large number of hyperparameters:\n",
    "\n",
    "- It can be difficult to \"manually tune\" models by inspecting\n",
    "  how performance changes across different values of the parameter\n",
    "- The search space gets very large: if you have $n_1$ values for one parameter, then $n_2$ and $n_3$ values for other parameters, a grid\n",
    "  search has to test $n_1 \\times n_2 \\times n_3$ parameter combinations\n",
    "\n",
    "To tune models with many hyperparameters, you can:\n",
    "\n",
    "* Use the university HPC (Artemis) to run a grid search. Since this works best when you can split a task into smaller individual jobs, you can construct\n",
    "  a grid of parameters, and for each individual parameter combination\n",
    "  (and possibly each cross validation fold!),\n",
    "  pass them to a script that fits the training data to that combination\n",
    "  and returns the score\n",
    "* Use a parameter tuning algorithm that can explore the search space automatically, and suggest/generate parameters that are closer\n",
    "  to the optimal parameters. Examples include [hyperopt](http://hyperopt.github.io/hyperopt/) and [optuna](https://optuna.org/)\n",
    "\n",
    "To demonstrate this, we will use the [optuna](https://optuna.org/) package to optimize the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic parameter tuning with optuna\n",
    "\n",
    "Tuning a model with `optuna` requires you to define the search\n",
    "space a bit differently to the grid search: instead of providing\n",
    "specific values, you have to define the random distributions\n",
    "to sample from during tuning.\n",
    "\n",
    "`optuna` requires you to define a function to fit the model\n",
    "for each parameter combination, and return the error metric. This is a bit more manual work than previous models, where `sklearn` has\n",
    "tools to automatically choose parameters based on an error metric.\n",
    "\n",
    "The difficult part of tuning the parameters is determining a range that's reasonable for each parameter - this requires some understanding of the model and how each parameter is used.\n",
    "\n",
    "> ### âš ï¸ Challenge: XGBoost parameters\n",
    "> Review the documentation about XGBoost's [parameters](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster).\n",
    "> Do the values defined here for the parameter search look sensible?\n",
    "> If you don't have enough knowledge to judge this yourself, where\n",
    "> could you look for guidance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(trial: optuna.Trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 1, 5),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 0, 1),\n",
    "        'reg_alpha' : trial.suggest_float(\"reg_alpha\", 0, 50),\n",
    "        'reg_lambda' : trial.suggest_float(\"reg_lambda\", 10, 100),\n",
    "        'colsample_bytree' : trial.suggest_float(\"colsample_bytree\", 0, 1),\n",
    "        'min_child_weight' : trial.suggest_float(\"min_child_weight\", 0, 5),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 50, 1000, log=True),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0, .15),\n",
    "        'max_bin' : trial.suggest_int(\"max_bin\", 50, 500)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, objective='reg:squarederror')\n",
    "    # NOTE: using early stopping like this to prevent overfitting\n",
    "    #   may require a \"validation\" dataset, separate from\n",
    "    #   both the training and test set. We just use the test\n",
    "    #   set here for illustration\n",
    "    evaluation = [(ames_test_x, ames_test_y)]\n",
    "    model.fit(ames_train_x, ames_train_y, \n",
    "              eval_set=evaluation, \n",
    "              eval_metric=\"rmse\",\n",
    "              early_stopping_rounds=100,\n",
    "              verbose=False)\n",
    "\n",
    "    #Obtain prediction and rmse score.\n",
    "    pred = model.predict(ames_test_x)\n",
    "    rmse = np.sqrt(mean_squared_error(ames_test_y, pred))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've defined the search space and the tuning function, you can start running the algorithm to search for the optimal parameters.\n",
    "\n",
    "For this workshop, we will only run a small number of iterations - definitely not enough to properly explore the parameter space.\n",
    "\n",
    "More iterations require more computing resources and more time. As a guide, running 5,000 trials took 40 minutes on a recent MacBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "\n",
    "# Reduce the amount of output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(optimize, n_trials=n_trials, show_progress_bar=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters we find with a small number of trials are unlikely to be optimal, so instead we'll use parameters we found by tuning for 5,000 trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned_params = {'max_depth': 3,\n",
    " 'gamma': 0.0001898945871315337,\n",
    " 'reg_alpha': 0.012021942260241877,\n",
    " 'reg_lambda': 14.972731190737358,\n",
    " 'colsample_bytree': 0.3436362920251812,\n",
    " 'min_child_weight': 1.6176495662230623,\n",
    " 'n_estimators': 999,\n",
    " 'learning_rate': 0.09497626888034086,\n",
    " 'max_bin': 62}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_xgb = xgb.XGBRegressor(**xgb_tuned_params, objective='reg:squarederror')\n",
    "ames_xgb.fit(ames_train_x, ames_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the `xgboost` model perform on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model_fit(models=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_rf, ames_knn, ames_xgb],\n",
    "    model_labels=['OLS','Ridge', 'Lasso', 'ENet', 'RF', 'kNN', \"XGB\"],\n",
    "    datasetX=ames_train_x,\n",
    "    datasetY=ames_train_y).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_model_fit(models=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_rf, ames_knn, ames_xgb],\n",
    "                 model_labels=['OLS','Ridge', 'Lasso', 'ENet', 'RF', 'kNN', \"XGB\"],\n",
    "                 datasetX=ames_test_x,\n",
    "                 datasetY=ames_test_y).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## â„¹ï¸ xgboost has a lot of options!\n",
    "> Note that the parameter tuning above still doesn't cover all the potential\n",
    "> options available in the xgboost model - there is also the option to try [different boosting algorithms](https://xgboost.readthedocs.io/en/stable/tutorials/dart.html), sometimes with their own unique parameters.\n",
    ">\n",
    "> We may not have maximised the performance of XGBoost in our example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
